{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix\n",
    "\n",
    "def load_data(file_path):\n",
    "    print(\"Loading data...\")\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Data loaded successfully.\")\n",
    "    return data\n",
    "\n",
    "def visualize_class_distribution(data, target_col='Score'):\n",
    "    print(\"Visualizing class distribution...\")\n",
    "    data[target_col].value_counts().plot(kind='bar', alpha=0.6)\n",
    "    plt.title(f'{target_col} Distribution')\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "def downsample_class(data, target_col='Score', class_value=5, downsample_frac=0.5):\n",
    "    print(\"Downsampling data to balance classes...\")\n",
    "    fives = data[data[target_col] == class_value].sample(frac=downsample_frac, random_state=42)\n",
    "    data = pd.concat([data[data[target_col] != class_value], fives], ignore_index=True)\n",
    "    print(\"Downsampling completed.\")\n",
    "    visualize_class_distribution(data)\n",
    "    return data\n",
    "\n",
    "def encode_categorical_features(data, columns):\n",
    "    print(\"Applying One-Hot Encoding...\")\n",
    "    OHE = OneHotEncoder(sparse_output=True)\n",
    "    encoded_data = OHE.fit_transform(data[columns])\n",
    "    print(\"One-Hot Encoding completed.\")\n",
    "    return encoded_data, OHE\n",
    "\n",
    "def preprocess_data(data):\n",
    "    print(\"Preprocessing data: Imputing and Standardizing...\")\n",
    "    data.fillna({'Text': '', 'Summary': ''}, inplace=True)\n",
    "    data['Helpful'] = data['HelpfulnessNumerator']\n",
    "    data['Unhelpful'] = data['HelpfulnessDenominator'] - data['HelpfulnessNumerator']\n",
    "    scaler = StandardScaler()\n",
    "    data[['Helpful', 'Unhelpful', 'Time']] = scaler.fit_transform(data[['Helpful', 'Unhelpful', 'Time']])\n",
    "    data.drop(['HelpfulnessDenominator', 'HelpfulnessNumerator'], axis=1, inplace=True)\n",
    "    print(\"Data preprocessing completed.\")\n",
    "    return data\n",
    "\n",
    "def calculate_tfidf(data, column):\n",
    "    print(f\"Calculating TF-IDF for {column} column...\")\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "    matrix = vectorizer.fit_transform(data[column])\n",
    "    print(f\"TF-IDF calculation for {column} completed.\")\n",
    "    return matrix, vectorizer\n",
    "\n",
    "def prepare_features(data, encoded_ids, text_matrix, summary_matrix):\n",
    "    print(\"Concatenating all feature matrices...\")\n",
    "    numerical = scipy.sparse.csr_matrix(data[['Helpful', 'Unhelpful', 'Time']].values)\n",
    "    X = scipy.sparse.hstack([text_matrix, summary_matrix, numerical, encoded_ids])\n",
    "    print(\"Feature concatenation completed.\")\n",
    "    return X\n",
    "\n",
    "def split_data(X, data, target_col='Score'):\n",
    "    print(\"Splitting data into training and testing sets...\")\n",
    "    mask = data[target_col].isnull()\n",
    "    ind_test = mask.to_numpy().nonzero()[0]\n",
    "    ind_train = (~mask).to_numpy().nonzero()[0]\n",
    "    train_X = scipy.sparse.csr_matrix(X)[ind_train]\n",
    "    test_X = scipy.sparse.csr_matrix(X)[ind_test]\n",
    "    train_Y = data[target_col].loc[~mask].reset_index(drop=True)\n",
    "    print(\"Data split completed.\")\n",
    "    return train_X, test_X, train_Y\n",
    "\n",
    "def extract_important_words(vectorizer, model):\n",
    "    print(\"Extracting important words...\")\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    coefficients = model.coef_.flatten()\n",
    "    \n",
    "    # Debugging check for length mismatch\n",
    "    if len(feature_names) != len(coefficients):\n",
    "        print(\"Mismatch in lengths:\")\n",
    "        print(\"Length of feature_names:\", len(feature_names))\n",
    "        print(\"Length of coefficients:\", len(coefficients))\n",
    "        return  # Exit to avoid the ValueError\n",
    "\n",
    "    important_words = pd.DataFrame({'word': feature_names, 'coefficient': coefficients})\n",
    "    important_words = important_words.reindex(important_words['coefficient'].abs().sort_values(ascending=False).index)\n",
    "    \n",
    "    # Display top positive and negative words\n",
    "    print(\"Top Positive Words:\", important_words.head(10))\n",
    "    print(\"Top Negative Words:\", important_words.tail(10))\n",
    "\n",
    "\n",
    "def cross_validate_logistic(k, X, y, vectorizer):\n",
    "    print(\"Starting cross-validation...\")\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=1)\n",
    "    total_mse = 0\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"Training fold {fold}/{k}...\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model = LogisticRegression(random_state=0, max_iter=1000)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_test_pred)\n",
    "        total_mse += mse\n",
    "        all_predictions.extend(y_test_pred)\n",
    "        all_true_labels.extend(y_test)\n",
    "        \n",
    "        print(f\"Fold {fold} MSE: {mse}\")\n",
    "    \n",
    "    average_mse = total_mse / k\n",
    "    print(\"Cross-validation completed.\")\n",
    "    print(\"Average MSE:\", average_mse)\n",
    "\n",
    "    # Confusion Matrix Visualization\n",
    "    cm = confusion_matrix(all_true_labels, all_predictions, normalize='true')\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.show()\n",
    "\n",
    "    extract_important_words(vectorizer, model)\n",
    "    return model, average_mse\n",
    "\n",
    "def main():\n",
    "    data = load_data('train.csv')\n",
    "    data = downsample_class(data)\n",
    "    encoded_ids, OHE = encode_categorical_features(data, ['ProductId', 'UserId'])\n",
    "    data = preprocess_data(data)\n",
    "    text_matrix, text_vectorizer = calculate_tfidf(data, 'Text')\n",
    "    summary_matrix, summary_vectorizer = calculate_tfidf(data, 'Summary')\n",
    "    X = prepare_features(data, encoded_ids, text_matrix, summary_matrix)\n",
    "    train_X, test_X, train_Y = split_data(X, data)\n",
    "    model, mse = cross_validate_logistic(10, train_X, train_Y, text_vectorizer)\n",
    "    predict_and_save(model, test_X)\n",
    "\n",
    "def predict_and_save(model, test_X):\n",
    "    print(\"Making predictions and saving to CSV...\")\n",
    "    df_test = pd.read_csv('test.csv')\n",
    "    sample = pd.read_csv('sample.csv')\n",
    "    predict_df = pd.DataFrame(sample)\n",
    "    predict_df['Score'] = model.predict(test_X)\n",
    "    predict_df.to_csv('predicted_scores_logistic.csv', index=False)\n",
    "    print(\"Predictions saved to 'predicted_scores_logistic.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
